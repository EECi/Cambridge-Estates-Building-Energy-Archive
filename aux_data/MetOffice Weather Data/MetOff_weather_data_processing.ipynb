{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and prepare Met Office weather data for use in CityLearn environment\n",
    "\n",
    "The weather variables required are: \"Outdoor Drybulb Temperature [C], Relative Humidity [%], Diffuse Solar Radiation [W/m2], Direct Solar Radiation [W/m2]\"\n",
    "\n",
    "I have decided to use:\n",
    "- the `bedford` station for temperatue and humidity, from this dataset https://catalogue.ceda.ac.uk/uuid/c9663d0c525f4b0698f1ec4beae3688e (cambridge-botanic-garden only has daily data, and cambridge-niab has terrible data availability)\n",
    "\n",
    "I will also process data from:\n",
    "- the `bedford` station for solar irradiation data, from this dataset https://catalogue.ceda.ac.uk/uuid/87eb67c08f5c4518a3723d0ca2d9b4b1 (unfortunately this station only has global irradiation data, and no direct-diffuse breakdown - no stations near Cambridge have this)\n",
    "\n",
    "for both `qc-version-1` data is used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative weather dataset is available from Meteostat, https://meteostat.net/en/station/EGSC0, however this data has worse availability, and so the MIDAS dataset is used by preference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on solar irradiance\n",
    "\n",
    "The Met Office MIDAS dataset only provides global irradiance values.\n",
    "\n",
    "However, renewables.ninja provides direct and diffuse components of solar irradiance which are used in its calculation of pv power generation. But, these values are dervied from MERRA-2 reanalysis data, as described in, https://www.sciencedirect.com/science/article/pii/S0360544216311744, and that reference states \"The global coverage of reanalysis data may come at the cost of accuracy [4]. examine two reanalyses (ERA-Interim and MERRA) and show that their irradiance values are less accurate than satellite-derived data, frequently predicting clear skies when the sky was cloudy.\"\n",
    "\n",
    "But, as these are the values that are used in the calculation of the solar generation power, they will likely have greater correlation to the power values (which are the variables of importance to the control problem), and so will provide better predictors.\n",
    "\n",
    "However, the irradiance values provided by renewables.ninja do not have a unit explicitly associated. Reading between the lines fo the documentation, they appear to be in [kW/m2] - see solar data processing ipynb for more detail.\n",
    "\n",
    "An alternative option would be to use the renewables.ninja data to get an estimate of the diffuse-to-direct irradiance ratio, and then combine that with the real measurement Met Office data to estimate the diffuse and direct components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2000,2023))\n",
    "in_dir = 'raw_data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep weather data (temperature & relative humidity)\n",
    "\n",
    "In the Met MIDAS dataset:\n",
    "- 'Outdoor Drybulb Temperature [C]' is given by variable `air_temperature`\n",
    "- 'Relative Humidity [%]' is given by variable `rltv_hum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do I want to get this automatically from the metadata csv file?\n",
    "weather_metadata = {\n",
    "    'station name': 'bedford',\n",
    "    'location': [52.227, -0.465],\n",
    "    'MetOff station id': 3560\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir = 'hourly_weather'\n",
    "in_fname_pattern = 'midas-open_uk-hourly-weather-obs_dv-202308_bedfordshire_00461_bedford_qcv-1_%s.csv'\n",
    "in_fpath_pattern = os.path.join(in_dir,subdir,in_fname_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_observations = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "    year_data = pd.read_csv(\n",
    "        in_fpath_pattern%year,\n",
    "        usecols=['ob_time','air_temperature','rltv_hum'],\n",
    "        skiprows=range(280),\n",
    "        skipfooter=1,\n",
    "        engine='python'\n",
    "        )\n",
    "    weather_observations = pd.concat([weather_observations,year_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ob_time  air_temperature  rltv_hum\n",
      "0       2000-01-01 00:00:00              8.1      94.3\n",
      "1       2000-01-01 01:00:00              8.2      95.8\n",
      "2       2000-01-01 02:00:00              8.1      97.2\n",
      "3       2000-01-01 03:00:00              8.2      97.2\n",
      "4       2000-01-01 04:00:00              8.0      97.2\n",
      "...                     ...              ...       ...\n",
      "199024  2022-12-31 19:00:00             11.3      94.3\n",
      "199025  2022-12-31 20:00:00             11.4      92.4\n",
      "199026  2022-12-31 21:00:00             11.6      88.1\n",
      "199027  2022-12-31 22:00:00             11.8      82.6\n",
      "199028  2022-12-31 23:00:00             11.2      82.9\n",
      "\n",
      "[199029 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(weather_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into dataframes for each variable\n",
    "temp_df = weather_observations[['ob_time','air_temperature']].copy()\n",
    "hum_df = weather_observations[['ob_time','rltv_hum']].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep solar data (global irradiation only)\n",
    "\n",
    "The only relevant variable available is `glbl_irad_amt`, which is the \"Global solar irradiation amount\" in [KJ/m2]\n",
    "\n",
    "According to the table in Section 4.1.4 of the documentation (https://zenodo.org/record/7357325), the \"Global radiation\" is \"Total global radiation over preceding hour\" in \"W hr/m2\" (this isn't right, it is in kJ/m2).\n",
    "So to produce the required [W/m2] value, the data needs to be mulitplied by $1000[J/kJ]/3600[s/hr]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do I want to get this automatically from the metadata csv file?\n",
    "solar_metadata = {\n",
    "    'station name': 'bedford',\n",
    "    'location': [52.227, -0.465],\n",
    "    'MetOff station id': 3440\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir = 'hourly_solar'\n",
    "fname_pattern = 'midas-open_uk-radiation-obs_dv-202308_bedfordshire_00461_bedford_qcv-1_%s.csv'\n",
    "fpath_pattern = os.path.join(in_dir,subdir,fname_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "irad_df = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "    year_data = pd.read_csv(\n",
    "        fpath_pattern%year,\n",
    "        usecols=['ob_end_time','id_type','glbl_irad_amt'],\n",
    "        skiprows=range(75),\n",
    "        skipfooter=1,\n",
    "        engine='python'\n",
    "        )\n",
    "    irad_df = pd.concat([irad_df,year_data],ignore_index=True)\n",
    "\n",
    "irad_df = irad_df[irad_df['id_type'] == 'DCNN']\n",
    "irad_df.drop('id_type',axis=1,inplace=True) # delete type col after filtering\n",
    "\n",
    "# convert to units of W/m2\n",
    "irad_df['glbl_irad_amt']  = np.around(irad_df['glbl_irad_amt']*(1000/3600), 1)\n",
    "\n",
    "# rename timestamps column for consistency with weather data\n",
    "irad_df.rename(columns={'ob_end_time':'ob_time'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ob_time  glbl_irad_amt\n",
      "0       2000-01-01 00:00:00            0.0\n",
      "1       2000-01-01 01:00:00            0.0\n",
      "2       2000-01-01 02:00:00            0.0\n",
      "3       2000-01-01 03:00:00            0.0\n",
      "4       2000-01-01 04:00:00            0.0\n",
      "...                     ...            ...\n",
      "198501  2022-12-31 19:00:00            0.0\n",
      "198502  2022-12-31 20:00:00            0.0\n",
      "198503  2022-12-31 21:00:00            0.0\n",
      "198504  2022-12-31 22:00:00            0.0\n",
      "198505  2022-12-31 23:00:00            0.0\n",
      "\n",
      "[198044 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(irad_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs = [temp_df, hum_df, irad_df]\n",
    "vars = ['air_temperature','rltv_hum','glbl_irad_amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_df(df, var):\n",
    "    \"\"\"Clean dataframe containing observations for a single data variable.\n",
    "    Strip out NaNs and fill observations gaps using linear interpolation,\n",
    "    of two kinds depending on the duration of the gap.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.Dataframe): Dataframe containing observation data for the variable\n",
    "        var (str): name of the variable column in the dataframe\n",
    "\n",
    "    Returns:\n",
    "        clean_df (pandas.Dataframe): dataframe of cleaned data\n",
    "    \"\"\"\n",
    "    # identify unavailable hours (either missing or NaN values)\n",
    "    df['ob_time'] = pd.to_datetime(df['ob_time'])\n",
    "    available_hours = list(df['ob_time'])\n",
    "    missing_hours = pd.date_range(start=available_hours[0], end=available_hours[-1], freq='h').difference(available_hours)\n",
    "    nan_hours = df[np.isnan(df[var])]['ob_time']\n",
    "\n",
    "    unav_hours = missing_hours.append(pd.DatetimeIndex(nan_hours))\n",
    "    unav_hours = unav_hours.sort_values()\n",
    "\n",
    "    unav_ranges = []\n",
    "    unav_range_durations = []\n",
    "    if len(unav_hours) > 0:\n",
    "        consec_unav = unav_hours[1:]-unav_hours[:-1]  == pd.Timedelta(hours=1)\n",
    "        start = unav_hours[0]\n",
    "        for t in range(len(unav_hours)-1):\n",
    "            if not consec_unav[t]:\n",
    "                unav_ranges.append([start,unav_hours[t]])\n",
    "                unav_range_durations.append(unav_hours[t]-start)\n",
    "                start = unav_hours[t+1]\n",
    "        unav_ranges.append([start,unav_hours[-1]])\n",
    "        unav_range_durations.append(unav_hours[-1]-start)\n",
    "    else:\n",
    "        print(\"No unavailable hours!\")\n",
    "\n",
    "    # remove times with NaN observations\n",
    "    clean_df = df[np.invert(np.isnan(df[var]))]\n",
    "    clean_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # clean data (fill in unavailable values)\n",
    "    insert_df = pd.DataFrame()\n",
    "\n",
    "    for (window, duration) in list(zip(unav_ranges, unav_range_durations)):\n",
    "\n",
    "        # for short durations use simple linear interpolation between adjacent times\n",
    "        if duration <= pd.Timedelta(4, 'hours'):\n",
    "            before_time = pd.Timestamp(window[0]) + pd.Timedelta(-1, 'hours')\n",
    "            before_time_index = clean_df.index[clean_df['ob_time']==before_time].to_list()[0]\n",
    "            after_time = pd.Timestamp(window[1]) + pd.Timedelta(1, 'hours')\n",
    "            after_time_index = clean_df.index[clean_df['ob_time']==after_time].to_list()[0]\n",
    "            gap_length = (after_time - before_time).total_seconds()\n",
    "\n",
    "            for tstamp in pd.date_range(start=window[0], end=window[1], freq='h'):\n",
    "                insert_obvs = {'ob_time':[tstamp]}\n",
    "                dx = (tstamp - before_time).total_seconds()\n",
    "                dy = (clean_df.loc[after_time_index][var] - clean_df.loc[before_time_index][var]) * dx/gap_length\n",
    "                insert_obvs[var] = clean_df.loc[before_time_index][var] + dy\n",
    "                insert_df = pd.concat([insert_df, pd.DataFrame(insert_obvs)])\n",
    "\n",
    "    clean_df = pd.concat([clean_df,insert_df], ignore_index=True)\n",
    "    clean_df.sort_values('ob_time', inplace=True, ignore_index=True)\n",
    "\n",
    "    insert_df = pd.DataFrame()\n",
    "\n",
    "    for (window, duration) in list(zip(unav_ranges, unav_range_durations)):\n",
    "\n",
    "        # for longer durations use linear interpolation between observed values for\n",
    "        # the same hour on the days with available data on either side.\n",
    "        if duration > pd.Timedelta(4, 'hours'):\n",
    "            # NOTE: this method only works when the data is hourly on the hour - can I think of a smarter,\n",
    "            # more general way to do this??\n",
    "            # Also, we assume that long duration unavailable windows are not adjacent, otherwise this breaks\n",
    "            for tstamp in pd.date_range(start=window[0], end=window[1], freq='h'):\n",
    "                insert_obvs = {'ob_time':[tstamp]}\n",
    "\n",
    "                h = tstamp.hour\n",
    "                # compute number of hours needed to go back from the window start to find timestamp with same hour on day before unavailable window\n",
    "                hours_back = pd.Timestamp(window[0]).hour - h if h < pd.Timestamp(window[0]).hour else 24 - (h - pd.Timestamp(window[0]).hour)\n",
    "                before_time = pd.Timestamp(window[0]) - pd.Timedelta(hours_back, 'hours')\n",
    "                # check if this time is available, if not step one day before until value available\n",
    "                while len(clean_df.index[clean_df['ob_time']==before_time].to_list()) == 0:\n",
    "                    before_time = before_time + pd.Timedelta(-1, 'days')\n",
    "                before_time_index = clean_df.index[clean_df['ob_time']==before_time].to_list()[0]\n",
    "\n",
    "                # compute number of hours needed to go forwards from the window end to find timestamp with same hour on day after unavailable window\n",
    "                hours_forward = h - pd.Timestamp(window[1]).hour if h > pd.Timestamp(window[1]).hour else 24 - (pd.Timestamp(window[1]).hour - h)\n",
    "                after_time = pd.Timestamp(window[1]) + pd.Timedelta(hours_forward, 'hours')\n",
    "                while len(clean_df.index[clean_df['ob_time']==after_time].to_list()) == 0:\n",
    "                    after_time = after_time + pd.Timedelta(1, 'days')\n",
    "                after_time_index = clean_df.index[clean_df['ob_time']==after_time].to_list()[0]\n",
    "\n",
    "                gap_length = (after_time - before_time).total_seconds()\n",
    "\n",
    "                dx = (tstamp - before_time).total_seconds()\n",
    "                dy = (clean_df.loc[after_time_index][var] - clean_df.loc[before_time_index][var]) * dx/gap_length\n",
    "                insert_obvs[var] = clean_df.loc[before_time_index][var] + dy\n",
    "                insert_df = pd.concat([insert_df, pd.DataFrame(insert_obvs)])\n",
    "\n",
    "    clean_df = pd.concat([clean_df,insert_df], ignore_index=True)\n",
    "    clean_df.sort_values('ob_time', inplace=True, ignore_index=True)\n",
    "\n",
    "    # recheck for unavailable hours\n",
    "    available_hours = list(clean_df['ob_time'])\n",
    "    missing_hours = pd.date_range(start=available_hours[0], end=available_hours[-1], freq='h').difference(available_hours)\n",
    "    nan_hours = clean_df[np.isnan(clean_df[var])]['ob_time']\n",
    "    assert len(missing_hours) == len(nan_hours) == 0\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "[clean_temp, clean_hum, clean_irad] = [clean_data_df(df, var) for df, var in zip(data_dfs, vars)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine observations and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(clean_temp['ob_time'], clean_hum['ob_time'])\n",
    "assert np.array_equal(clean_temp['ob_time'], clean_irad['ob_time'])\n",
    "# this requires the first and last day of the desired year range to have\n",
    "# available data for all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine cleaned data into a single dataframe\n",
    "metoff_data = clean_temp.copy()\n",
    "metoff_data['rltv_hum'] = clean_hum['rltv_hum']\n",
    "metoff_data['glbl_irad_amt'] = clean_irad['glbl_irad_amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renamed columns to provide variables units for clarity\n",
    "metoff_data.rename(columns={\n",
    "    'ob_time':'datetime',\n",
    "    'air_temperature':'air_temperature [degC]',\n",
    "    'rltv_hum':'rltv_hum [%]',\n",
    "    'glbl_irad_amt':'glbl_irad_amt [W/m2]'\n",
    "    },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metoff_data = metoff_data.round({'air_temperature [degC]':1, 'rltv_hum [%]':1, 'glbl_irad_amt [W/m2]':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed data to csv by year\n",
    "location = 'bedford' # - get from metadata?\n",
    "save_fname = 'MetOff_weather-{0}_mdata.json'.format(location)\n",
    "save_dir = os.path.join('processed_data',location)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for year in years:\n",
    "    year_data = metoff_data[metoff_data['datetime'].dt.year == year].copy()\n",
    "    year_data['datetime'] = year_data['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    year_data.to_csv(os.path.join(save_dir,f'{year}.csv'), index=False)\n",
    "\n",
    "# save metadata\n",
    "mdata_save_path = os.path.join(save_dir,save_fname)\n",
    "mdata_dict = {\n",
    "    'location':location,\n",
    "    'data_path':save_dir,\n",
    "    'years':years,\n",
    "    'weather_metdata':weather_metadata,\n",
    "    'solar_metadata':solar_metadata\n",
    "    }\n",
    "with open(mdata_save_path, 'w') as file:\n",
    "    json.dump(mdata_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19cf59d1263503ae45811c0afc42e401269b1675e31fff20a39b49ff6c64a80e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
