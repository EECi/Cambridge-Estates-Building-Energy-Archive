{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Usage in CityLearn from Pre-Processed Data CSVs (from sub-dirs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note: in the data preparation process, all datetime stamps are given in `%Y-%m-%d %H:%M:%S` format, and all variables are given as the value of the quantity over the period (hour) beginning at the timestamp. Note that CityLearn uses the hour 24 convention (24th hour of previous day), but Python standard timestamps use hour 0. So once data has been collected, it is then converted to hour 24 format for savings as a CityLearn compatible dataset (though note only the building CSVs contain time information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure dataset to be constructed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE! Remember to check asset capacity overwriting cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset info\n",
    "dataset_name = 'v2-test'\n",
    "vnum = '2-0' # version number\n",
    "dataset_notes = '...'\n",
    "\n",
    "elec_only = False\n",
    "battery_efficiency = 0.9\n",
    "\n",
    "# specify buildings to include in dataset - by ID\n",
    "building_ids = [0,2,120]\n",
    "\n",
    "# specify years of data to include in dataset\n",
    "metering_years = [2019,2020,2022]\n",
    "pricing_years = [2021] # available: 2019 to 2023\n",
    "carbon_years = [2021] # available: 2009 to 2023\n",
    "# NOTE: pricing_years and carbon_years are extended by repetition to match the length of metering_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metering_years = sorted(metering_years)\n",
    "pricing_years = sorted(pricing_years)\n",
    "carbon_years = sorted(carbon_years)\n",
    "dt_stamps = [ts for y in metering_years for ts in pd.date_range(start=pd.Timestamp(day=1,month=1,year=y,hour=0), end=pd.Timestamp(day=31,month=12,year=y,hour=23), freq='h')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to data files to use\n",
    "data_sources = {\n",
    "    'cebd_path_pattern': os.path.join('building_data','processed_data','UCam_Building_b%s'),\n",
    "    'building_ids': [f'b{n}' for n in building_ids],\n",
    "    'floor_roof_areas': os.path.join('building_data','processed_data','building_floor_roof_areas.csv'),\n",
    "    'weather_path': os.path.join('aux_data','MetOffice Weather Data','processed_data','bedford'),\n",
    "    'solar_path': os.path.join('aux_data','RenewablesNinja Generation Data','processed_data','cambridge_52-194_0-131'),\n",
    "    'pricing_path': os.path.join('aux_data','AgileOctopus Pricing Data','processed_data','import_price_regA'),\n",
    "    'carbon_intensity_path': os.path.join('aux_data','NationalGridESO Carbon Intensity Data','processed_data')\n",
    "}\n",
    "\n",
    "# specify metadata for dataset\n",
    "metadata = {\n",
    "    # !!! edit this\n",
    "    'name': dataset_name,\n",
    "    'version': vnum,\n",
    "    'metering_data_years': metering_years,\n",
    "    'pricing_data_years': pricing_years,\n",
    "    'carbon_data_years': carbon_years,\n",
    "    'timezone': 'Europe/London', # from valid pytz.all_timezones\n",
    "    'time_created': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'notes': dataset_notes,\n",
    "    'data_source_info': data_sources\n",
    "}\n",
    "metadata.update({'save_dir': os.path.join('datasets','{0}-v{1}'.format(metadata['name'],metadata['version']))})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check building load data is available for requested years\n",
    "for year in metering_years:\n",
    "    for b_id in building_ids:\n",
    "        assert os.path.exists(os.path.join(data_sources['cebd_path_pattern']%b_id,'electricity',f'{year}.csv')), 'No eletricity data for building b%d in year %d' % (b_id,year)\n",
    "        if not elec_only:\n",
    "            assert os.path.exists(os.path.join(data_sources['cebd_path_pattern']%b_id,'gas',f'{year}.csv')), 'No gas data for building b%d in year %d' % (b_id,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if weather data available for all metering years\n",
    "unav_weather_years  = []\n",
    "for year in metering_years:\n",
    "    if not os.path.exists(os.path.join(data_sources['weather_path'],f'{year}.csv')):\n",
    "        unav_weather_years.append(year)\n",
    "\n",
    "if len(unav_weather_years) > 0:\n",
    "    print(f'WARNING: Weather data not available for years: {unav_weather_years}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "elec_load_dfs = [pd.concat([\n",
    "    pd.read_csv(os.path.join(data_sources['cebd_path_pattern']%b_id,'electricity',f'{year}.csv')) for year in metering_years\n",
    "    ], ignore_index=True) for b_id in building_ids]\n",
    "if not elec_only:\n",
    "    heat_load_dfs = [pd.concat([\n",
    "        pd.read_csv(os.path.join(data_sources['cebd_path_pattern']%b_id,'gas',f'{year}.csv')) for year in metering_years\n",
    "        ], ignore_index=True) for b_id in building_ids]\n",
    "\n",
    "weather_df = pd.concat([\n",
    "    pd.read_csv(os.path.join(data_sources['weather_path'],f'{year}.csv'), usecols=['datetime','air_temperature [degC]','rltv_hum [%]']\n",
    "                ) for year in metering_years if year not in unav_weather_years], ignore_index=True)\n",
    "null_weather_dfs = [pd.DataFrame({\n",
    "    'datetime': pd.date_range(\n",
    "        start=pd.Timestamp(day=1,month=1,year=year,hour=0),\n",
    "        end=pd.Timestamp(day=31,month=12,year=year,hour=23), freq='h'\n",
    "        ),\n",
    "    'air_temperature [degC]': 20, # placeholder temp\n",
    "    'rltv_hum [%]': 0 # placeholder humidity\n",
    "    }) for year in unav_weather_years]\n",
    "weather_df = pd.concat([weather_df,*null_weather_dfs], ignore_index=True)\n",
    "\n",
    "solar_df = pd.concat([\n",
    "    pd.read_csv(\n",
    "        os.path.join(data_sources['solar_path'],f'{year}.csv'),\n",
    "        usecols=['datetime','solar generation [W/kW]','irradiance_direct [W/m2]','irradiance_diffuse [W/m2]']\n",
    "        ) for year in metering_years], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df_to_year(df, target_year, variable):\n",
    "    \"\"\"Adjust year of df timestamps and account for leap years by removing/repeating data.\"\"\"\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    current_year = df['datetime'].dt.year.unique()[0]\n",
    "\n",
    "    if (not pd.Timestamp(year=current_year,month=1,day=1).is_leap_year) and (pd.Timestamp(year=target_year,month=1,day=1).is_leap_year):\n",
    "        # repeat Feb28 data to Feb29\n",
    "        feb28_values = df[(df['datetime'].dt.month==2) & (df['datetime'].dt.day==28)][variable].to_list()\n",
    "        feb29_timestamps = pd.date_range(start=pd.Timestamp(day=29,month=2,year=target_year,hour=0), end=pd.Timestamp(day=29,month=2,year=target_year,hour=23), freq='h')\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: x.replace(year=target_year)) # change year to match target year to allow merge\n",
    "        df = pd.concat([df,pd.DataFrame({'datetime': feb29_timestamps,variable: feb28_values})], ignore_index=True)\n",
    "\n",
    "    elif (pd.Timestamp(year=current_year,month=1,day=1).is_leap_year) and (not pd.Timestamp(year=target_year,month=1,day=1).is_leap_year):\n",
    "        # remove Feb29 pricing data\n",
    "        df = df.drop(df[(df['datetime'].dt.month==2) & (df['datetime'].dt.day==29)].index)\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: x.replace(year=target_year)) # change year to match target year\n",
    "    else:\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: x.replace(year=target_year)) # change year to match target year\n",
    "\n",
    "    df.sort_values(by='datetime', inplace=True, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend pricing and carbon data, matching leap years\n",
    "extended_price_years = list(itertools.islice(itertools.cycle(pricing_years),len(metering_years)))\n",
    "pricing_year_dfs = []\n",
    "for pyear,myear in zip(extended_price_years,metering_years):\n",
    "    pyear_df = pd.read_csv(os.path.join(data_sources['pricing_path'],f'{pyear}.csv'))\n",
    "    pricing_year_dfs.append(fit_df_to_year(pyear_df,myear,'Electricity Pricing [Â£/kWh]'))\n",
    "pricing_df = pd.concat(pricing_year_dfs, ignore_index=True)\n",
    "pricing_df['Electricity Pricing [Â£/kWh]'] = pricing_df['Electricity Pricing [Â£/kWh]'].clip(0) # clip prices at zero\n",
    "\n",
    "extended_carbon_years = list(itertools.islice(itertools.cycle(carbon_years),len(metering_years)))\n",
    "carbon_year_dfs = []\n",
    "for cyear,myear in zip(extended_carbon_years,metering_years):\n",
    "    cyear_df = pd.read_csv(os.path.join(data_sources['carbon_intensity_path'],f'{cyear}.csv'))\n",
    "    carbon_year_dfs.append(fit_df_to_year(cyear_df,myear,'Carbon Intensity [kg_CO2/kWh]'))\n",
    "carbon_df = pd.concat(carbon_year_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all dfs have required length\n",
    "all_dfs = [*elec_load_dfs,*heat_load_dfs,weather_df,solar_df,pricing_df,carbon_df]\n",
    "df_names = [*[f'elec_b{b_id}' for b_id in building_ids],*[f'heat_b{b_id}' for b_id in building_ids],'weather','solar','pricing','carbon']\n",
    "\n",
    "for df in all_dfs:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "for df,name in zip(all_dfs,df_names):\n",
    "    assert len(df) == len(dt_stamps), f'Dataframe length mismatch for df {name}, {len(df)} != {len(dt_stamps)}'\n",
    "# they should all have the same timestamps as well\n",
    "for df,name in zip(all_dfs,df_names):\n",
    "    assert (df['datetime'] == dt_stamps).all(), f'Timestamps do not match for df {name}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data sources into required files (dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct time information for buildings\n",
    "times_df = pd.DataFrame(data=[pd.Timestamp(ts,tz='UTC') for ts in dt_stamps], columns=['timestamp'])\n",
    "times_df['month'] = times_df['timestamp'].dt.month\n",
    "times_df['day of week'] = times_df['timestamp'].dt.dayofweek\n",
    "times_df['hour'] = times_df['timestamp'].dt.hour\n",
    "times_df['daysave'] = [1 if (ts.astimezone(tz=metadata['timezone']).dst() == pd.Timedelta(1,'h')) else 0 for ts in times_df['timestamp'].to_list()]\n",
    "# convert to EnergyPlus day numbering convention - Sun -> 1, ... , Sat -> 7\n",
    "# https://bigladdersoftware.com/epx/docs/8-1/input-output-reference/page-009.html\n",
    "# note, I will ignore 'special' days (type 8)\n",
    "times_df['day of week'] += 2\n",
    "times_df['day of week'] = times_df['day of week'].replace(8,1)\n",
    "# convert to hour 24 format - i.e. midnight is 24th hour of previous day\n",
    "times_df['hour'] = times_df['hour'].replace(0,24)\n",
    "times_df.loc[times_df['hour'] == 24, 'day of week'] -= 1\n",
    "times_df['day of week'] = times_df['day of week'].replace(0,7)\n",
    "times_df.loc[[(ts.is_month_start and hr == 24) for ts,hr in zip(times_df['timestamp'],times_df['hour'])], 'month'] -= 1\n",
    "times_df['month'] = times_df['month'].replace(0,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_files = []\n",
    "for elec_df,heat_df,id in zip(elec_load_dfs,heat_load_dfs,data_sources['building_ids']):\n",
    "    building_file = times_df[['month','hour','day of week','daysave']].copy()\n",
    "    building_file.rename(columns={'month':'Month','hour':'Hour','day of week':'Day Type','daysave':'Daylight Savings Status'},inplace=True)\n",
    "    building_file['Indoor Temperature [C]'] = \"\"\n",
    "    building_file['Average Unmet Cooling Setpoint Difference [C]'] = \"\"\n",
    "    building_file['Indoor Relative Humidity [%]'] = \"\"\n",
    "    building_file['Equipment Electric Power [kWh]'] = elec_df['equipment load [kWh]']\n",
    "    building_file['DHW Heating [kWh]'] = 0\n",
    "    building_file['Cooling Load [kWh]'] = 0\n",
    "    building_file['Heating Load [kWh]'] = 0 if elec_only else heat_df['heating load [kWh]']\n",
    "    building_file['Solar Generation [W/kW]'] = solar_df['solar generation [W/kW]']\n",
    "\n",
    "    building_files.append(building_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_intensity_file = pd.DataFrame(data=carbon_df['Carbon Intensity [kg_CO2/kWh]'].to_list(),columns=['kg_CO2/kWh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_file = pd.DataFrame(data=pricing_df['Electricity Pricing [Â£/kWh]'].to_list(),columns=['Electricity Pricing [Â£/kWh]'])\n",
    "for dt in [6,12,24]:\n",
    "    pricing_file['%sh Prediction Electricity Pricing [Â£/kWh]'%dt] = pricing_file['Electricity Pricing [Â£/kWh]'].shift(periods=-dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cols = ['Outdoor Drybulb Temperature [C]',\n",
    "    'Relative Humidity [%]',\n",
    "    'Diffuse Solar Radiation [W/m2]',\n",
    "    'Direct Solar Radiation [W/m2]']\n",
    "\n",
    "weather_file = pd.DataFrame(data=np.array([\n",
    "    weather_df['air_temperature [degC]'].to_list(),\n",
    "    weather_df['rltv_hum [%]'].to_list(),\n",
    "    solar_df['irradiance_diffuse [W/m2]'].to_list(),\n",
    "    solar_df['irradiance_direct [W/m2]'].to_list()\n",
    "    ]).T,\n",
    "    columns=weather_cols)\n",
    "\n",
    "for col_name in weather_cols:\n",
    "    for dt in [6,12,24]:\n",
    "        weather_file[('%sh Prediction '%dt + col_name)] = weather_file[col_name].shift(periods=-dt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data writing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_fname_pattern = 'UCam_Building_b%s.csv'\n",
    "carbon_fname = 'carbon_intensity.csv'\n",
    "pricing_fname = 'pricing.csv'\n",
    "weather_fname = 'weather.csv'\n",
    "timedata_fname = 'timestamps.csv'\n",
    "schema_fname = 'schema.json'\n",
    "mdata_fname = 'metadata.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `schema.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get building area attributes\n",
    "floor_roof_areas_df = pd.read_csv(data_sources['floor_roof_areas'],usecols=['Building ID','GIA (m2)','Number of floors'])\n",
    "\n",
    "floor_areas = {}\n",
    "num_floors = {}\n",
    "approx_roof_areas = {}\n",
    "for b_id in data_sources['building_ids']:\n",
    "    floor_areas[b_id] = np.round(*floor_roof_areas_df[floor_roof_areas_df['Building ID'] == b_id]['GIA (m2)'].values,1)\n",
    "    num_floors[b_id] = float(floor_roof_areas_df[floor_roof_areas_df['Building ID'] == b_id]['Number of floors'].values[0])\n",
    "    approx_roof_areas[b_id] = np.round(floor_areas[b_id]/num_floors[b_id],1)\n",
    "\n",
    "floor_areas,num_floors,approx_roof_areas = [dict(sorted(d.items(), key=lambda item: item[0])) for d in (floor_areas,num_floors,approx_roof_areas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional pre-processing to decide how to size the batteries & pv panels\n",
    "\n",
    "storage_hours = 24 # no. of hours of mean load that can be stored in battery\n",
    "storage_power_factor = 3 # proportion of mean load that battery can provide as output power\n",
    "#solar_power_factor = 1 # proportion of mean load that pv panel can provide as peak power\n",
    "panel_fill_factor = 0.9 # proportion of roof filled by PV panels\n",
    "panel_power_density = 0.15 # (kWp/m2) panel peak (nominal) power per area\n",
    "# panel refs: https://www.sciencedirect.com/science/article/pii/S0378778822002547 (Table 1), https://iopscience.iop.org/article/10.1088/1748-9326/aaa554/meta (Table 1), https://www.pvfitcalculator.energysavingtrust.org.uk/Documents/150224_SolarEnergy_Calculator_Sizing_Guide_v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b0': 22.4, 'b120': 11.4, 'b2': 871.4}\n",
      "{'b0': 538.0, 'b120': 274.0, 'b2': 20914.0}\n",
      "{'b0': 67.0, 'b120': 34.0, 'b2': 2614.0}\n",
      "{'b0': 89.0, 'b120': 162.0, 'b2': 342.0}\n"
     ]
    }
   ],
   "source": [
    "# set schema building parameters\n",
    "battery_efficiencies = {id: battery_efficiency for id in sorted(data_sources['building_ids'])}\n",
    "\n",
    "id_file_zip = sorted(list(zip(building_files,data_sources['building_ids'])), key=lambda x: x[1])\n",
    "mean_loads = {id: np.round(np.mean(building_file['Equipment Electric Power [kWh]']),1) for building_file,id in id_file_zip}\n",
    "battery_energy_capacities = {id: np.round(storage_hours*mean_loads[id],0) for id in sorted(data_sources['building_ids'])}\n",
    "battery_power_capacities = {id: np.round(storage_power_factor*mean_loads[id],0) for id in sorted(data_sources['building_ids'])}\n",
    "#pv_power_capacities = {id: np.round(solar_power_factor*mean_loads[id],0) for id in sorted(data_sources['building_ids'])}\n",
    "pv_power_capacities = {id: np.round(approx_roof_areas[id]*panel_fill_factor*panel_power_density,0) for id in sorted(data_sources['building_ids'])}\n",
    "\n",
    "print(mean_loads)\n",
    "print(battery_energy_capacities)\n",
    "print(battery_power_capacities)\n",
    "print(pv_power_capacities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override building parameters for consistency between train, test, validate sets (set using train data)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_mdata = {\n",
    "    'mean_loads (kW)': mean_loads,\n",
    "    'battery_efficiencies (-)': battery_efficiencies,\n",
    "    'battery_energy_capacities (kWh)': battery_energy_capacities,\n",
    "    'battery_power_capacities (kW)': battery_power_capacities,\n",
    "    'pv_power_capacities (kW)': pv_power_capacities,\n",
    "    'gross_internal_floor_area (m2)': floor_areas,\n",
    "    'number_of_floors (-)': num_floors,\n",
    "    'approx_roof_area (m2)': approx_roof_areas\n",
    "}\n",
    "metadata.update({'building_attributes': building_mdata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base schema\n",
    "with open(os.path.join('resources','base_schema.json')) as base_schema:\n",
    "    schema = json.load(base_schema)\n",
    "\n",
    "schema['simulation_end_time_step'] = len(dt_stamps) - 1 # set length of simulation\n",
    "\n",
    "# write building attributes\n",
    "schema['buildings'] = {}\n",
    "for id in sorted(data_sources['building_ids']):\n",
    "\n",
    "    int_id = id.replace('b','')\n",
    "\n",
    "    building_dict = {\n",
    "        'include': True,\n",
    "        'energy_simulation': building_fname_pattern%int_id,\n",
    "        'weather': weather_fname,\n",
    "        'carbon_intensity': carbon_fname,\n",
    "        'pricing': pricing_fname,\n",
    "        'inactive_observations': [],\n",
    "        'inactive_actions': [],\n",
    "\n",
    "        'electrical_storage': {\n",
    "            'type': \"citylearn.energy_model.Battery\",\n",
    "            'autosize': False,\n",
    "            'attributes': {\n",
    "                'capacity': battery_energy_capacities[id],\n",
    "                'nominal_power': battery_power_capacities[id],\n",
    "                'capacity_loss_coefficient': 1e-05,\n",
    "                'loss_coefficient': 0,\n",
    "                'power_efficiency_curve': [[0,battery_efficiencies[id]],[1,battery_efficiencies[id]]],\n",
    "                'capacity_power_curve': [[0,1],[1,1]]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        'pv': {\n",
    "            'type': \"citylearn.energy_model.PV\",\n",
    "            'autosize': False,\n",
    "            'attributes': {\n",
    "                'nominal_power': pv_power_capacities[id]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    schema['buildings'].update({building_fname_pattern%int_id: building_dict})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'v2-test', 'version': '2-0', 'metering_data_years': [2019, 2020, 2022], 'pricing_data_years': [2021], 'carbon_data_years': [2021], 'timezone': 'Europe/London', 'time_created': '2024-04-09 14:31:44', 'notes': '...', 'data_source_info': {'cebd_path_pattern': 'building_data/processed_data/UCam_Building_b%s', 'building_ids': ['b0', 'b2', 'b120'], 'floor_roof_areas': 'building_data/processed_data/building_floor_roof_areas.csv', 'weather_path': 'aux_data/Met Office Weather Data/processed_data/bedford', 'solar_path': 'aux_data/RenewablesNinja Generation Data/processed_data/cambridge_52-194_0-131', 'pricing_path': 'aux_data/AgileOctopus Pricing Data/processed_data/import_price_regA', 'carbon_intensity_path': 'aux_data/NationalGridESO Carbon Intensity Data/processed_data'}, 'save_dir': 'datasets/v2-test-v2-0', 'building_attributes': {'mean_loads (kW)': {'b0': 22.4, 'b120': 11.4, 'b2': 871.4}, 'battery_efficiencies (-)': {'b0': 0.9, 'b120': 0.9, 'b2': 0.9}, 'battery_energy_capacities (kWh)': {'b0': 538.0, 'b120': 274.0, 'b2': 20914.0}, 'battery_power_capacities (kW)': {'b0': 67.0, 'b120': 34.0, 'b2': 2614.0}, 'pv_power_capacities (kW)': {'b0': 89.0, 'b120': 162.0, 'b2': 342.0}, 'gross_internal_floor_area (m2)': {'b0': 3296.5, 'b120': 2401.5, 'b2': 25319.8}, 'number_of_floors (-)': {'b0': 5.0, 'b120': 2.0, 'b2': 10.0}, 'approx_roof_area (m2)': {'b0': 659.3, 'b120': 1200.8, 'b2': 2532.0}}}\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframes & metadata\n",
    "if not os.path.exists(metadata['save_dir']):\n",
    "    os.mkdir(metadata['save_dir'])\n",
    "\n",
    "for building_file,id in  zip(building_files,building_ids):\n",
    "    building_file.to_csv(os.path.join(metadata['save_dir'],building_fname_pattern%id), index=False)\n",
    "\n",
    "carbon_intensity_file.to_csv(os.path.join(metadata['save_dir'],carbon_fname), index=False)\n",
    "pricing_file.to_csv(os.path.join(metadata['save_dir'],pricing_fname), index=False)\n",
    "weather_file.to_csv(os.path.join(metadata['save_dir'],weather_fname), index=False)\n",
    "\n",
    "with open(os.path.join(metadata['save_dir'],timedata_fname), 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Timestamp (UTC)'])\n",
    "    writer.writerows([[tstamp] for tstamp in dt_stamps])\n",
    "\n",
    "with open(os.path.join(metadata['save_dir'],schema_fname), 'w') as file:\n",
    "    json.dump(schema, file, indent=4)\n",
    "\n",
    "with open(os.path.join(metadata['save_dir'],mdata_fname), 'w') as file:\n",
    "    json.dump(metadata, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
